{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ee7b0-839a-485f-b59c-8e02d1916876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from typing import (\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Optional,\n",
    "    Tuple,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358de03f-8745-4070-ad50-75f93a25f9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download tfrecords to local machine\n",
    "train_data = tfds.load(<dataset>, split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc66394-9b43-4e00-bdcf-5d3f2b0d6abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset(\n",
    "    filepaths: str,\n",
    "    feature_spec: dict,\n",
    "    is_training: bool,\n",
    "    image_size: int,\n",
    "    batch_size: int = 2,\n",
    "    block_length: int = 2,\n",
    "    max_seq_len: int = 12,\n",
    "    subsample_size: Optional[int] = None,\n",
    "    shuffle_buffer_size: int = 100,\n",
    "    tpu: bool = False,\n",
    "    prefetch_dataset_buffer_size: int = 8 * 1024 * 1024,\n",
    "    num_classes: Optional[int] = None,\n",
    "    use_bfloat16: bool = False,\n",
    "    deterministic: bool = False,\n",
    "    multi_gpu: bool = False,\n",
    ") -> tf.data.Dataset:\n",
    "\n",
    "    if deterministic:\n",
    "        files = tf.data.Dataset.list_files(filepaths, shuffle=False)\n",
    "    else:\n",
    "        files = tf.data.Dataset.list_files(filepaths, shuffle=True)\n",
    "\n",
    "    if is_training and not deterministic:\n",
    "        files = files.repeat()\n",
    "\n",
    "    def prefetch_dataset(filename):\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filename, buffer_size=prefetch_dataset_buffer_size\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    \"\"\"\n",
    "    TPU Performance guidance:\n",
    "    1. Avoid using AUTOTUNE as it could cause unrestricted use of host memory, instead use a fixed number. \n",
    "    Changing num_parallel_calls=tf.data.AUTOTUNE to a fixed number\n",
    "    2. prefetch_dataset_buffer_size will be overwritten to 127MB regardless of size specified if using GCS, so limit number of open files\n",
    "    3. Ensure tfrecord size is between 100-200MB when using GCS\n",
    "    4. avoid using DATA shard policy as this will unnecessary create buffers. Instead use default\n",
    "    Changing tf.data.experimental.AutoShardPolicy.DATA to tf.data.experimental.AutoShardPolicy.DEFAULT\n",
    "    \"\"\"\n",
    "    ds = files.interleave(\n",
    "        prefetch_dataset,\n",
    "        block_length=block_length,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True if deterministic else False,\n",
    "    )\n",
    "    if subsample_size and subsample_size > 0:\n",
    "        ds = ds.take(subsample_size)\n",
    "    if is_training and not deterministic:\n",
    "        ds = ds.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if multi_gpu or not is_training:\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = (\n",
    "            tf.data.experimental.AutoShardPolicy.DEFAULT\n",
    "        )\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if use_bfloat16:\n",
    "        img_dtype = tf.float16 if multi_gpu else tf.bfloat16\n",
    "    else:\n",
    "        img_dtype = tf.float32\n",
    "    ds = ds.map(\n",
    "        lambda x: parse_single_example(\n",
    "            x,\n",
    "            feature_spec,\n",
    "            image_size,\n",
    "            img_dtype\n",
    "        ),\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True if deterministic else None,\n",
    "    )\n",
    "\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82447150-6f80-4893-b495-f78bf6a39801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_single_example(\n",
    "    example_proto: tf.Tensor,\n",
    "    feature_spec: dict,\n",
    "    image_size,\n",
    "    image_dtype,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=example_proto,\n",
    "        features=feature_spec,\n",
    "    )\n",
    "    image = tf.io.decode_jpeg(parsed_features[\"image\"], channels=3)\n",
    "    image = tf.image.resize(\n",
    "        [image], [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC\n",
    "    )[0]\n",
    "    image = tf.reshape(image, [image_size, image_size, 3])\n",
    "    image = tf.image.convert_image_dtype(\n",
    "        image, dtype=image_dtype\n",
    "    )\n",
    "    image = image / 255.\n",
    "\n",
    "    return image, parsed_features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f075c7e-2838-4bee-b32c-ede53db6d72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_fps = f\"/home/jupyter/tensorflow_datasets/<datapath>/train.tfrecord*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc170c4f-7df1-4d36-9cab-e9aff41ed2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_SPEC = {\n",
    "    \"image\": tf.io.FixedLenFeature((), tf.string, \"\"),\n",
    "    \"label\": tf.io.FixedLenFeature((), tf.int64, -1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f98698-091c-4693-9bb6-2f86ace20e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9d295e-e804-44a5-993f-fd1d2b41db23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TPU Performance guidance:\n",
    "1. If num records in tfrecord is large, limiting batch size will help reduce buffer size\n",
    "changing 4 to 1 as number of files open is very large for prefetch\n",
    "\"\"\"\n",
    "\n",
    "training_dataset = get_preprocessed_dataset(\n",
    "        filepaths=train_fps,\n",
    "        feature_spec=FEATURE_SPEC,\n",
    "        is_training=True,\n",
    "        batch_size=1,\n",
    "        block_length=2,\n",
    "        prefetch_dataset_buffer_size=4,\n",
    "        max_seq_len=0,\n",
    "        subsample_size=None,\n",
    "        shuffle_buffer_size=4,\n",
    "        tpu=False,\n",
    "        image_size=img_size,\n",
    "        num_classes=2,\n",
    "        use_bfloat16=False,\n",
    "        deterministic=False,\n",
    "        multi_gpu=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7560217-d383-415b-be8b-bf7b1ae5664b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# didn't split properly for simplicity; only focused on data pipeline\n",
    "valid_fps = f\"/home/jupyter/tensorflow_datasets/<datapath>/train.tfrecord-00000-of-00016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a068c492-8712-4373-80dd-03883373f23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TPU Performance guidance:\n",
    "1. If num records in tfrecord is large, limiting batch size will help reduce buffer size\n",
    "changing 4 to 1 as number of files open is very large for prefetch\n",
    "\"\"\"\n",
    "\n",
    "validation_dataset = get_preprocessed_dataset(\n",
    "        filepaths=train_fps,\n",
    "        feature_spec=FEATURE_SPEC,\n",
    "        is_training=False,\n",
    "        batch_size=1,\n",
    "        block_length=2,\n",
    "        prefetch_dataset_buffer_size=4,\n",
    "        max_seq_len=0,\n",
    "        subsample_size=None,\n",
    "        shuffle_buffer_size=4,\n",
    "        tpu=False,\n",
    "        image_size=img_size,\n",
    "        num_classes=2,\n",
    "        use_bfloat16=False,\n",
    "        deterministic=False,\n",
    "        multi_gpu=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff962a6-5431-4f00-a2df-de8120982a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(300, 300, 3)),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d142c-d69c-479a-9fe7-c7b2c1532349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2baf564b-528d-40b5-b824-ef8f7552cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_training_dataset = strategy.experimental_distribute_dataset(\n",
    "    training_dataset.repeat()\n",
    ")\n",
    "distributed_validation_dataset = strategy.experimental_distribute_dataset(\n",
    "    validation_dataset.repeat()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01935221-9f21-4dd5-b00f-83031c99dd8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "steps_per_epoch=25000//batch_size\n",
    "validation_steps=390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281c802-acb6-444c-8614-03d93f184ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    distributed_training_dataset,\n",
    "    batch_size=batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=distributed_validation_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
